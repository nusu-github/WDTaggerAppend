"""Custom augmentation transforms for WD Tagger training.

The module houses standalone torchvision v2 transforms and factory helpers that assemble
augmentation pipelines matching the original TensorFlow preprocessing used to train WD Tagger
models. The utilities here are reused by both training and evaluation entrypoints.
"""

from __future__ import annotations

from typing import TYPE_CHECKING, Annotated, Any, TypedDict

import torch
import torch.nn.functional as F
from torch.utils.data import default_collate
from torchvision.transforms import InterpolationMode as TorchvisionInterpolationMode, v2
from torchvision.transforms.v2 import functional as TF
from transformers import AutoConfig, BaseImageProcessor, BatchFeature
from transformers.image_utils import (
    ImageInput,
    make_flat_list_of_images,
    valid_images,
    validate_kwargs,
)
from transformers.utils import logging

logger = logging.get_logger(__name__)

if TYPE_CHECKING:
    from collections.abc import Callable, Sequence

    from transformers.utils.generic import TensorType


class BatchItem(TypedDict):
    pixel_values: torch.Tensor
    labels: torch.Tensor


class PadParams(TypedDict):
    padding: list[int]
    needs_padding: bool


class ToColorWithCheckIncluded(torch.nn.Module):
    def forward(self, image: torch.Tensor) -> torch.Tensor:
        """Convert grayscale image to RGB format if needed.

        Args:
            image: Input image tensor.

        Returns:
            Image in RGB format as Tensor.
        """
        if image.shape[0] == 1:
            # Convert grayscale to RGB by repeating channels
            image = image.repeat(3, 1, 1)
        elif image.shape[0] == 2:
            # Convert grayscale + alpha to RGBA by repeating channels
            rgb = image[0:1, :, :].repeat(3, 1, 1)
            alpha = image[1:2, :, :]
            image = torch.cat((rgb, alpha), dim=0)
        return image


class ToBGR(torch.nn.Module):
    def forward(self, image: torch.Tensor) -> torch.Tensor:
        """Convert RGB image to BGR format.

        Args:
            image: Input image tensor in RGB format.

        Returns:
            Image in BGR format as Tensor.
        """
        return TF.permute_channels(image, permutation=[2, 1, 0])


class PadToSquare(v2.Transform):
    def __init__(
        self,
        fill: v2._utils._FillType | dict[type | str, v2._utils._FillType] = 0,
    ) -> None:
        """Initialize the transform with the desired padding fill value.

        Args:
            fill: Padding value forwarded to ``torchvision.transforms.v2`` utilities.
        """
        super().__init__()
        self.fill = v2._utils._setup_fill_arg(fill)

    def make_params(self, flat_inputs: list[Any]) -> Annotated[dict[str, Any], PadParams]:
        """Compute symmetric padding required to make the image square.

        Args:
            flat_inputs: Flattened batch of inputs provided by torchvision.

        Returns:
            Padding metadata consumed by :meth:`transform`.
        """
        _, height, width = v2._utils.query_chw(flat_inputs[0])
        max_side = max(height, width)
        pad_height = max_side - height
        pad_width = max_side - width
        padding = [
            pad_width // 2,
            pad_height // 2,
            pad_width - pad_width // 2,
            pad_height - pad_height // 2,
        ]
        needs_padding = any(p > 0 for p in padding)
        return {"padding": padding, "needs_padding": needs_padding}

    def transform(self, inpt: Any, params: Annotated[dict[str, Any], PadParams]) -> Any:
        """Apply the computed padding when the image is not already square.

        Args:
            inpt: Input image to pad.
            params: Pre-computed padding metadata generated by :meth:`make_params`.

        Returns:
            Image tensor or PIL image padded to square dimensions.
        """
        if not params["needs_padding"]:
            return inpt

        fill = v2._utils._get_fill(self.fill, type(inpt))
        return TF.pad(inpt, padding=params["padding"], fill=fill)


class RgbaToRgbWithWhiteBackground(torch.nn.Module):
    """Convert RGBA image to RGB by alpha-blending with white background.

    This transform converts RGBA images to RGB by blending transparent areas
    with a white background using proper alpha compositing. Batched tensors are
    vectorized directly without per-sample Python loops.
    Images without alpha channel are returned as-is (first 3 channels only). Both
    single images shaped ``(C, H, W)`` and batched tensors shaped ``(B, C, H, W)``
    are supported.

    Note:
        This transform assumes input images are already scaled to [0, 1] range.
    """

    def forward(self, image: torch.Tensor) -> torch.Tensor:
        """Apply alpha blending with white background.

        Args:
            image: Input RGBA image tensor of shape (4, H, W) or (B, 4, H, W),
                   assumed to be in [0, 1] range.

        Returns:
            RGB image tensor with white background blended.
        """
        was_batched = image.ndim == 4
        if not was_batched:
            image = image.unsqueeze(0)

        C = image.shape[1]
        if C < 3:
            msg = "expected at least 3 channels"
            raise ValueError(msg)
        if C == 3:
            out = image[:, :3, :, :]
        elif C == 4:
            rgb = image[:, :3, :, :]
            alpha = image[:, 3:4, :, :]
            out = rgb * alpha + (1.0 - alpha)
        else:
            msg = "expected 3 or 4 channels"
            raise ValueError(msg)
        return out if was_batched else out.squeeze(0)


class AreaResize(torch.nn.Module):
    """Resize transform using area interpolation (adaptive average pooling).

    This transform is equivalent to TensorFlow's tf.image.resize with method='area'.
    It uses adaptive average pooling which implicitly applies antialiasing.

    Args:
        size: Desired output size. If size is a sequence like (h, w), output size
            will be matched to this. If size is an int, smaller edge of the image
            will be matched to this number maintaining aspect ratio.
        antialias: This parameter is ignored for area mode as antialiasing is
            implicit in the averaging operation.
    """

    def __init__(
        self,
        size: int | Sequence[int],
    ) -> None:
        """Initialize AreaResize transform."""
        super().__init__()
        if isinstance(size, int):
            self.size = (size, size)
        else:
            self.size = tuple(size)

    def forward(self, image: torch.Tensor) -> torch.Tensor:
        """Apply area interpolation resize to the input.

        Args:
            image: Input image tensor of shape (C, H, W) or (B, C, H, W).

        Returns:
            Resized image as Tensor.
        """
        was_batched = image.ndim == 4
        if not was_batched:
            image = image.unsqueeze(0)
        image = F.interpolate(image, size=self.size, mode="area")
        return image if was_batched else image.squeeze(0)


def _load_image_processor_stats(
    pretrained_model_name_or_path: str,
) -> tuple[int, Sequence[float], Sequence[float]]:
    """Load the image processor metadata required to build transform pipelines.

    Args:
        pretrained_model_name_or_path: Name or path forwarded to Hugging Face ``AutoConfig``.

    Returns:
        Tuple containing the target image size, channel-wise mean, and standard deviation.

    Raises:
        ValueError: If the configuration omits the expected normalization metadata.
    """
    config = AutoConfig.from_pretrained(pretrained_model_name_or_path)

    try:
        pretrained_cfg = config.pretrained_cfg
    except AttributeError as exc:
        error_hint = (
            "Configuration loaded from "
            f"'{pretrained_model_name_or_path}' does not expose the 'pretrained_cfg' attribute "
            "that carries image normalization metadata."
        )
        raise ValueError(error_hint) from exc

    required_keys = ("input_size", "mean", "std")
    missing_keys = [key for key in required_keys if key not in pretrained_cfg]
    if missing_keys:
        msg = f"Configuration metadata is missing required keys: {', '.join(missing_keys)}."
        raise ValueError(
            msg,
        )

    input_size = pretrained_cfg["input_size"]
    if len(input_size) < 2:
        msg = "Configuration metadata must provide at least height and width in 'input_size'."
        raise ValueError(
            msg,
        )

    return int(input_size[1]), pretrained_cfg["mean"], pretrained_cfg["std"]


def create_random_interpolation_resize(
    size: int | Sequence[int],
) -> v2.RandomChoice:
    """Create a RandomChoice transform for random interpolation resize.

    This matches TensorFlow's random_resize_method behavior where interpolation
    method is randomly chosen from area, bilinear, and bicubic to make the model
    more resilient to different image resizing implementations.

    Args:
        size: Desired output size. If size is a sequence like (h, w), output size
            will be matched to this. If size is an int, output will be (size, size).

    Returns:
        RandomChoice transform that randomly selects between area, bilinear, and bicubic.
    """
    # Create resize transforms with different interpolation modes
    resize_transforms = [
        AreaResize(size=size),  # Area interpolation (adaptive average pooling)
        v2.Resize(
            size=size,
            interpolation=TorchvisionInterpolationMode.BILINEAR,
            antialias=True,
        ),
        v2.Resize(
            size=size,
            interpolation=TorchvisionInterpolationMode.BICUBIC,
            antialias=True,
        ),
    ]

    # Use RandomChoice to randomly select one of the resize methods
    # This uses torch.rand() for randomness, which respects torch.manual_seed()
    return v2.RandomChoice(resize_transforms)


def create_train_transform(
    pretrained_model_name_or_path: str,
    random_crop_scale: tuple[float, float] = (0.87, 0.998),
    rotation_degrees: float = 0.0,
    cutout_scale: tuple[float, float] = (0.02, 0.1),
    cutout_ratio: tuple[float, float] = (0.3, 3.3),
    random_resize_method: bool = True,
) -> v2.Compose:
    """Create the training augmentation pipeline that mirrors the TensorFlow implementation.

    Args:
        pretrained_model_name_or_path: Name or path forwarded to Hugging Face ``AutoConfig``.
        random_crop_scale: Scale range for ``RandomResizedCrop`` (min, max).
        rotation_degrees: Maximum rotation angle in degrees. Set to 0 to disable.
        cutout_scale: Area proportion range supplied to ``RandomErasing``.
        cutout_ratio: Aspect ratio range supplied to ``RandomErasing``.
        random_resize_method: Whether to add an additional resize with random interpolation.

    Returns:
        Composed transform pipeline.

    Raises:
        ValueError: If the referenced configuration omits image normalization metadata.
    """
    image_size, mean, std = _load_image_processor_stats(pretrained_model_name_or_path)

    transform_steps = [
        v2.ToImage(),
        v2.ToDtype(torch.float32, scale=True),  # Scale to [0, 1] for alpha blending
        ToColorWithCheckIncluded(),  # Convert grayscale to RGB/RGBA if needed
        RgbaToRgbWithWhiteBackground(),  # Convert RGBA to RGB with white background
        PadToSquare([1.0, 1.0, 1.0]),  # Pad to square with white background
        v2.RandomHorizontalFlip(p=0.5),
        v2.RandomResizedCrop(
            size=image_size,
            scale=random_crop_scale,
            interpolation=TorchvisionInterpolationMode.BILINEAR,
            antialias=True,
        ),
    ]

    # Add random interpolation resize if enabled
    if random_resize_method:
        transform_steps.append(create_random_interpolation_resize(size=image_size))

    # Add rotation if enabled
    if rotation_degrees > 0:
        transform_steps.append(
            v2.RandomRotation(
                degrees=(-rotation_degrees, rotation_degrees),
                interpolation=TorchvisionInterpolationMode.BILINEAR,
                fill=1.0,  # White fill for boundary
            ),
        )

    # Add random erasing (cutout)
    transform_steps.append(
        v2.RandomErasing(
            p=0.5,
            scale=cutout_scale,
            ratio=cutout_ratio,
            value=0.5,  # Gray fill matching TensorFlow implementation
        ),
    )

    # Convert to float and normalize
    transform_steps.extend(
        [
            v2.Normalize(
                mean=mean,
                std=std,
            ),  # Normalize to match TF: (x/127.5)-1
            ToBGR(),  # Convert RGB to BGR
        ],
    )

    return v2.Compose(transform_steps)


def create_eval_transform(
    pretrained_model_name_or_path: str,
) -> v2.Compose:
    """Create evaluation/validation transform pipeline.

    Args:
        pretrained_model_name_or_path: Name or path forwarded to Hugging Face ``AutoConfig``.

    Returns:
        Composed transform pipeline.

    Raises:
        ValueError: If the referenced configuration omits image normalization metadata.
    """
    image_size, mean, std = _load_image_processor_stats(pretrained_model_name_or_path)

    return v2.Compose(
        [
            v2.ToImage(),
            v2.ToDtype(torch.float32, scale=True),  # Scale to [0, 1] for alpha blending
            ToColorWithCheckIncluded(),  # Convert grayscale to RGB/RGBA if needed
            RgbaToRgbWithWhiteBackground(),  # Convert RGBA to RGB with white background
            PadToSquare([1.0, 1.0, 1.0]),  # Pad to square with white background
            AreaResize(size=image_size),  # Use area interpolation for evaluation
            v2.Normalize(mean=mean, std=std),
            ToBGR(),  # Convert RGB to BGR
        ],
    )


class WDTaggerImageProcessor(BaseImageProcessor):
    """Image processor that mirrors WD Tagger preprocessing pipelines.

    The processor reuses the torchvision v2 augmentation helpers defined in this
    module to provide train-time and eval-time transform stacks that match the
    original TensorFlow implementation used to train WD Tagger models.
    """

    model_input_names = ["pixel_values"]

    def __init__(
        self,
        pretrained_model_name_or_path: str,
        do_train_augmentations: bool = False,
        random_crop_scale: tuple[float, float] = (0.87, 0.998),
        rotation_degrees: float = 0.0,
        cutout_scale: tuple[float, float] = (0.02, 0.1),
        cutout_ratio: tuple[float, float] = (0.3, 3.3),
        random_resize_method: bool = True,
        **kwargs,
    ) -> None:
        super().__init__(**kwargs)
        self.pretrained_model_name_or_path = pretrained_model_name_or_path
        self.random_crop_scale = random_crop_scale
        self.rotation_degrees = rotation_degrees
        self.cutout_scale = cutout_scale
        self.cutout_ratio = cutout_ratio
        self.random_resize_method = random_resize_method
        self.do_train_augmentations = do_train_augmentations

        image_size, mean, std = _load_image_processor_stats(pretrained_model_name_or_path)
        self.image_size = image_size
        self.image_mean = tuple(mean)
        self.image_std = tuple(std)
        self.size = {"height": image_size, "width": image_size}

        self._train_transform: v2.Compose | None = None
        self._eval_transform: v2.Compose | None = None

        self._valid_processor_keys = [
            "images",
            "do_train_augmentations",
            "return_tensors",
        ]

    def _get_train_transform(self) -> v2.Compose:
        if self._train_transform is None:
            self._train_transform = create_train_transform(
                pretrained_model_name_or_path=self.pretrained_model_name_or_path,
                random_crop_scale=self.random_crop_scale,
                rotation_degrees=self.rotation_degrees,
                cutout_scale=self.cutout_scale,
                cutout_ratio=self.cutout_ratio,
                random_resize_method=self.random_resize_method,
            )
        return self._train_transform

    def _get_eval_transform(self) -> v2.Compose:
        if self._eval_transform is None:
            self._eval_transform = create_eval_transform(
                pretrained_model_name_or_path=self.pretrained_model_name_or_path,
            )
        return self._eval_transform

    def preprocess(  # pyright: ignore[reportIncompatibleMethodOverride]
        self,
        images: ImageInput,
        do_train_augmentations: bool | None = None,
        return_tensors: str | TensorType | None = None,
        **kwargs,
    ) -> BatchFeature:
        validate_kwargs(
            captured_kwargs=list(kwargs.keys()),
            valid_processor_keys=self._valid_processor_keys,
        )

        use_train = (
            self.do_train_augmentations
            if do_train_augmentations is None
            else do_train_augmentations
        )

        transform = self._get_train_transform() if use_train else self._get_eval_transform()

        images = self.fetch_images(images)  # type: ignore
        flat_images = make_flat_list_of_images(images)

        if not flat_images:
            msg = "No images were provided for preprocessing."
            raise ValueError(msg)

        if not valid_images(flat_images):
            msg = "Invalid image type. Expected PIL, numpy array, or torch tensor inputs."
            raise ValueError(msg)

        processed = [transform(image) for image in flat_images]  # type: ignore

        if use_train:
            logger.debug("Applied training augmentations in WDTaggerImageProcessor.")

        tensor_type_name = getattr(return_tensors, "value", return_tensors)
        if tensor_type_name == "pt":
            data = {"pixel_values": torch.stack(processed)}
        else:
            data = {"pixel_values": processed}
        return BatchFeature(data=data, tensor_type=return_tensors)


def create_mixup_collate_fn(
    num_classes: int,
    alpha: float = 0.8,
) -> Callable[[list[BatchItem]], tuple[torch.Tensor, torch.Tensor]]:
    """Create a collate function with MixUp augmentation using torchvision v2.MixUp.

    This function creates a custom collate_fn for DataLoader that:
    1. Collates batches of samples into tensors
    2. Applies MixUp augmentation using torchvision's built-in implementation

    Args:
        num_classes: Number of classes for one-hot encoding labels
        alpha: Alpha parameter for Beta distribution in MixUp (default: 0.8)
            If alpha <= 0, no mixup is applied

    Returns:
        Collate function suitable for DataLoader.

    Example:
        >>> from torch.utils.data import DataLoader
        >>> collate_fn = create_mixup_collate_fn(num_classes=1000, alpha=0.8)
        >>> dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)
        >>> for images, labels in dataloader:
        ...     # images and labels already have MixUp applied
        ...     outputs = model(images)
        ...     loss = criterion(outputs, labels)

    Note:
        - Labels will be automatically one-hot encoded if they are 1D (index-based)
        - The returned labels are always 2D: (batch_size, num_classes)
        - Use with loss functions that accept soft labels (e.g., cross_entropy)
    """
    if alpha <= 0:
        # No MixUp, just use default collation
        return default_collate

    # Create MixUp transform
    mixup = v2.MixUp(num_classes=num_classes, alpha=alpha)

    def collate_fn(batch: list[BatchItem]) -> tuple[torch.Tensor, torch.Tensor]:
        """Collate batch with MixUp augmentation.

        Args:
            batch: List of samples, each containing 'pixel_values' and 'labels'

        Returns:
            Tuple of (images, labels) with MixUp applied.
        """
        # Extract images and labels
        images = torch.stack([item["pixel_values"] for item in batch])
        labels = torch.stack([item["labels"] for item in batch])

        # Apply MixUp (handles one-hot encoding internally if labels are 1D)
        images, labels = mixup(images, labels)

        return images, labels

    return collate_fn
