"""Custom augmentation transforms for WD Tagger training.

The module houses standalone torchvision v2 transforms and factory helpers that assemble
augmentation pipelines matching the original TensorFlow preprocessing used to train WD Tagger
models. The utilities here are reused by both training and evaluation entrypoints.
"""

from __future__ import annotations

from typing import TYPE_CHECKING, Annotated, Any, TypedDict

import torch
import torch.nn.functional as F
from torch.utils.data import default_collate
from torchvision.transforms import InterpolationMode as TorchvisionInterpolationMode, v2
from torchvision.transforms.v2 import functional as TF
from transformers import AutoConfig

if TYPE_CHECKING:
    from collections.abc import Callable, Sequence


class BatchItem(TypedDict):
    pixel_values: torch.Tensor
    labels: torch.Tensor


class PadParams(TypedDict):
    padding: list[int]
    needs_padding: bool


class ToBGR(v2.Transform):
    def transform(self, inpt: Any, params: dict[str, Any]) -> Any:  # noqa: ARG002
        """Convert RGB image to BGR format.

        Args:
            inpt: Input image (Tensor, PIL Image, or other compatible format)
            params: Transform parameters (unused)

        Returns:
            Image in BGR format as Tensor.
        """
        return TF.permute_channels(inpt, permutation=[2, 1, 0])


class PadToSquare(v2.Transform):
    def __init__(
        self,
        fill: v2._utils._FillType | dict[type | str, v2._utils._FillType] = 0,
    ) -> None:
        """Initialize the transform with the desired padding fill value.

        Args:
            fill: Padding value forwarded to ``torchvision.transforms.v2`` utilities.
        """
        super().__init__()
        self.fill = v2._utils._setup_fill_arg(fill)

    def make_params(self, flat_inputs: list[Any]) -> Annotated[dict[str, Any], PadParams]:
        """Compute symmetric padding required to make the image square.

        Args:
            flat_inputs: Flattened batch of inputs provided by torchvision.

        Returns:
            Padding metadata consumed by :meth:`transform`.
        """
        _, height, width = v2._utils.query_chw(flat_inputs[0])
        max_side = max(height, width)
        pad_height = max_side - height
        pad_width = max_side - width
        padding = [
            pad_width // 2,
            pad_height // 2,
            pad_width - pad_width // 2,
            pad_height - pad_height // 2,
        ]
        needs_padding = any(p > 0 for p in padding)
        return {"padding": padding, "needs_padding": needs_padding}

    def transform(self, inpt: Any, params: Annotated[dict[str, Any], PadParams]) -> Any:
        """Apply the computed padding when the image is not already square.

        Args:
            inpt: Input image to pad.
            params: Pre-computed padding metadata generated by :meth:`make_params`.

        Returns:
            Image tensor or PIL image padded to square dimensions.
        """
        if not params["needs_padding"]:
            return inpt

        fill = v2._utils._get_fill(self.fill, type(inpt))
        return TF.pad(inpt, padding=params["padding"], fill=fill)


class RgbaToRgbWithWhiteBackground(v2.Transform):
    """Convert RGBA image to RGB by alpha-blending with white background.

    This transform converts RGBA images to RGB by blending transparent areas
    with a white background using proper alpha compositing. Batched tensors are
    vectorized directly without per-sample Python loops.
    Images without alpha channel are returned as-is (first 3 channels only). Both
    single images shaped ``(C, H, W)`` and batched tensors shaped ``(B, C, H, W)``
    are supported.
    """

    def __init__(self) -> None:
        """Initialize the RgbaToRgbWithWhiteBackground transform."""
        super().__init__()

    def transform(self, inpt: Any, params: dict[str, Any]) -> Any:  # noqa: ARG002
        """Apply alpha blending with white background.

        Args:
            inpt: Input RGBA image tensor of shape (4, H, W)
            params: Transform parameters (unused)

        Returns:
            RGB image tensor of shape (3, H, W) with white background blended.
        """
        # Convert to tensor if needed
        img_tensor = TF.to_image(inpt) if not isinstance(inpt, torch.Tensor) else inpt

        if img_tensor.ndim == 3:
            return self._convert_single_image(img_tensor)

        if img_tensor.ndim == 4:
            return self._convert_batched_images(img_tensor)

        msg = (
            "RgbaToRgbWithWhiteBackground expects an image tensor with shape (C, H, W) "
            "or a batched tensor with shape (B, C, H, W)."
        )
        raise ValueError(msg)

    @staticmethod
    def _convert_single_image(image: torch.Tensor) -> torch.Tensor:
        """Convert a single image tensor from RGBA to RGB if needed."""
        if image.shape[0] != 4:
            # If no alpha channel, just return RGB
            return image[:3, :, :]

        # Split RGB and alpha channels
        rgb = image[:3, :, :]  # (3, H, W)
        alpha = image[3:4, :, :]  # (1, H, W)

        # Normalize alpha to [0, 1]
        alpha_norm = TF.to_dtype(alpha, torch.float32, scale=True)

        # Create white background
        max_val = 1.0 if rgb.is_floating_point() else 255
        white_background = torch.full_like(rgb, fill_value=max_val)

        # Convert to float for blending (TF.to_dtype handles no-op when already float).
        rgb_float = TF.to_dtype(rgb, torch.float32, scale=True)
        background_float = TF.to_dtype(white_background, torch.float32, scale=True)

        # Alpha blending: result = foreground * alpha + background * (1 - alpha).
        result = rgb_float * alpha_norm + background_float * (1.0 - alpha_norm)

        # Convert back to the original dtype if needed.
        return TF.to_dtype(result, image.dtype, scale=True)

    @staticmethod
    def _convert_batched_images(images: torch.Tensor) -> torch.Tensor:
        """Convert a batch of image tensors from RGBA to RGB if needed."""
        if images.shape[1] != 4:
            # If no alpha channel, just return RGB
            return images[:, :3, :, :]

        # Split RGB and alpha channels
        rgb = images[:, :3, :, :]  # (B, 3, H, W)
        alpha = images[:, 3:4, :, :]  # (B, 1, H, W)

        # Normalize alpha to [0, 1]
        alpha_norm = TF.to_dtype(alpha, torch.float32, scale=True)

        # Create white background
        max_val = 1.0 if rgb.is_floating_point() else 255
        white_background = torch.full_like(rgb, fill_value=max_val)

        # Convert to float for blending (TF.to_dtype handles no-op when already float).
        rgb_float = TF.to_dtype(rgb, torch.float32, scale=True)
        background_float = TF.to_dtype(white_background, torch.float32, scale=True)

        # Alpha blending: result = foreground * alpha + background * (1 - alpha).
        result = rgb_float * alpha_norm + background_float * (1.0 - alpha_norm)

        # Convert back to the original dtype if needed.
        return TF.to_dtype(result, images.dtype, scale=True)


class AreaResize(v2.Transform):
    """Resize transform using area interpolation (adaptive average pooling).

    This transform is equivalent to TensorFlow's tf.image.resize with method='area'.
    It uses adaptive average pooling which implicitly applies antialiasing.

    Args:
        size: Desired output size. If size is a sequence like (h, w), output size
            will be matched to this. If size is an int, smaller edge of the image
            will be matched to this number maintaining aspect ratio.
        antialias: This parameter is ignored for area mode as antialiasing is
            implicit in the averaging operation.
    """

    def __init__(
        self,
        size: int | Sequence[int],
    ) -> None:
        """Initialize AreaResize transform."""
        super().__init__()
        if isinstance(size, int):
            self.size = (size, size)
        else:
            self.size = tuple(size)

    def transform(self, inpt: Any, params: dict[str, Any]) -> Any:  # noqa: ARG002
        """Apply area interpolation resize to the input.

        Args:
            inpt: Input image (Tensor, PIL Image, or other compatible format)
            params: Transform parameters (unused)

        Returns:
            Resized image as Tensor.
        """
        if not isinstance(inpt, torch.Tensor):
            inpt = TF.to_image(inpt)

        dtype = inpt.dtype
        inpt = TF.to_dtype(inpt, torch.float32)

        needs_unsqueeze = inpt.ndim == 3
        if needs_unsqueeze:
            inpt = inpt.unsqueeze(0)

        # Apply area interpolation
        inpt = F.interpolate(inpt, size=self.size, mode="area")

        if needs_unsqueeze:
            inpt = inpt.squeeze(0)

        return TF.to_dtype(inpt, dtype)


def _load_image_processor_stats(
    pretrained_model_name_or_path: str,
) -> tuple[int, Sequence[float], Sequence[float]]:
    """Load the image processor metadata required to build transform pipelines.

    Args:
        pretrained_model_name_or_path: Name or path forwarded to Hugging Face ``AutoConfig``.

    Returns:
        Tuple containing the target image size, channel-wise mean, and standard deviation.

    Raises:
        ValueError: If the configuration omits the expected normalization metadata.
    """
    config = AutoConfig.from_pretrained(pretrained_model_name_or_path)

    try:
        pretrained_cfg = config.pretrained_cfg
    except AttributeError as exc:
        error_hint = (
            "Configuration loaded from "
            f"'{pretrained_model_name_or_path}' does not expose the 'pretrained_cfg' attribute "
            "that carries image normalization metadata."
        )
        raise ValueError(error_hint) from exc

    required_keys = ("input_size", "mean", "std")
    missing_keys = [key for key in required_keys if key not in pretrained_cfg]
    if missing_keys:
        msg = f"Configuration metadata is missing required keys: {', '.join(missing_keys)}."
        raise ValueError(
            msg,
        )

    input_size = pretrained_cfg["input_size"]
    if len(input_size) < 2:
        msg = "Configuration metadata must provide at least height and width in 'input_size'."
        raise ValueError(
            msg,
        )

    return int(input_size[1]), pretrained_cfg["mean"], pretrained_cfg["std"]


def create_random_interpolation_resize(
    size: int | Sequence[int],
) -> v2.RandomChoice:
    """Create a RandomChoice transform for random interpolation resize.

    This matches TensorFlow's random_resize_method behavior where interpolation
    method is randomly chosen from area, bilinear, and bicubic to make the model
    more resilient to different image resizing implementations.

    Args:
        size: Desired output size. If size is a sequence like (h, w), output size
            will be matched to this. If size is an int, output will be (size, size).

    Returns:
        RandomChoice transform that randomly selects between area, bilinear, and bicubic.
    """
    # Create resize transforms with different interpolation modes
    resize_transforms = [
        AreaResize(size=size),  # Area interpolation (adaptive average pooling)
        v2.Resize(
            size=size,
            interpolation=TorchvisionInterpolationMode.BILINEAR,
            antialias=True,
        ),
        v2.Resize(
            size=size,
            interpolation=TorchvisionInterpolationMode.BICUBIC,
            antialias=True,
        ),
    ]

    # Use RandomChoice to randomly select one of the resize methods
    # This uses torch.rand() for randomness, which respects torch.manual_seed()
    return v2.RandomChoice(resize_transforms)


def create_train_transform(
    pretrained_model_name_or_path: str,
    random_crop_scale: tuple[float, float] = (0.87, 0.998),
    rotation_degrees: float = 0.0,
    cutout_scale: tuple[float, float] = (0.02, 0.1),
    cutout_ratio: tuple[float, float] = (0.3, 3.3),
    random_resize_method: bool = True,
) -> v2.Compose:
    """Create the training augmentation pipeline that mirrors the TensorFlow implementation.

    Args:
        pretrained_model_name_or_path: Name or path forwarded to Hugging Face ``AutoConfig``.
        random_crop_scale: Scale range for ``RandomResizedCrop`` (min, max).
        rotation_degrees: Maximum rotation angle in degrees. Set to 0 to disable.
        cutout_scale: Area proportion range supplied to ``RandomErasing``.
        cutout_ratio: Aspect ratio range supplied to ``RandomErasing``.
        random_resize_method: Whether to add an additional resize with random interpolation.

    Returns:
        Composed transform pipeline.

    Raises:
        ValueError: If the referenced configuration omits image normalization metadata.
    """
    image_size, mean, std = _load_image_processor_stats(pretrained_model_name_or_path)

    transform_steps = [
        v2.ToImage(),
        RgbaToRgbWithWhiteBackground(),  # Convert RGBA to RGB with white background
        PadToSquare([255, 255, 255]),  # Pad to square with white background
        v2.RandomHorizontalFlip(p=0.5),
        v2.RandomResizedCrop(
            size=image_size,
            scale=random_crop_scale,
            interpolation=TorchvisionInterpolationMode.BILINEAR,
            antialias=True,
        ),
    ]

    # Add random interpolation resize if enabled
    if random_resize_method:
        transform_steps.append(create_random_interpolation_resize(size=image_size))

    # Add rotation if enabled
    if rotation_degrees > 0:
        transform_steps.append(
            v2.RandomRotation(
                degrees=(-rotation_degrees, rotation_degrees),
                interpolation=TorchvisionInterpolationMode.BILINEAR,
                fill=255,  # White fill for boundary
            ),
        )

    # Add random erasing (cutout)
    transform_steps.append(
        v2.RandomErasing(
            p=0.5,
            scale=cutout_scale,
            ratio=cutout_ratio,
            value=127,  # Gray fill matching TensorFlow implementation
        ),
    )

    # Convert to float and normalize
    transform_steps.extend(
        [
            v2.ToDtype(torch.float32, scale=True),  # [0, 255] -> [0, 1]
            v2.Normalize(
                mean=mean,
                std=std,
            ),  # Normalize to match TF: (x/127.5)-1
            ToBGR(),  # Convert RGB to BGR
        ],
    )

    return v2.Compose(transform_steps)


def create_eval_transform(
    pretrained_model_name_or_path: str,
) -> v2.Compose:
    """Create evaluation/validation transform pipeline.

    Args:
        pretrained_model_name_or_path: Name or path forwarded to Hugging Face ``AutoConfig``.

    Returns:
        Composed transform pipeline.

    Raises:
        ValueError: If the referenced configuration omits image normalization metadata.
    """
    image_size, mean, std = _load_image_processor_stats(pretrained_model_name_or_path)

    return v2.Compose(
        [
            v2.ToImage(),
            RgbaToRgbWithWhiteBackground(),  # Convert RGBA to RGB with white background
            PadToSquare([255, 255, 255]),  # Pad to square with white background
            AreaResize(size=image_size),  # Use area interpolation for evaluation
            v2.ToDtype(torch.float32, scale=True),
            v2.Normalize(mean=mean, std=std),
            ToBGR(),  # Convert RGB to BGR
        ],
    )


def create_mixup_collate_fn(
    num_classes: int,
    alpha: float = 0.8,
) -> Callable[[list[BatchItem]], tuple[torch.Tensor, torch.Tensor]]:
    """Create a collate function with MixUp augmentation using torchvision v2.MixUp.

    This function creates a custom collate_fn for DataLoader that:
    1. Collates batches of samples into tensors
    2. Applies MixUp augmentation using torchvision's built-in implementation

    Args:
        num_classes: Number of classes for one-hot encoding labels
        alpha: Alpha parameter for Beta distribution in MixUp (default: 0.8)
            If alpha <= 0, no mixup is applied

    Returns:
        Collate function suitable for DataLoader.

    Example:
        >>> from torch.utils.data import DataLoader
        >>> collate_fn = create_mixup_collate_fn(num_classes=1000, alpha=0.8)
        >>> dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)
        >>> for images, labels in dataloader:
        ...     # images and labels already have MixUp applied
        ...     outputs = model(images)
        ...     loss = criterion(outputs, labels)

    Note:
        - Labels will be automatically one-hot encoded if they are 1D (index-based)
        - The returned labels are always 2D: (batch_size, num_classes)
        - Use with loss functions that accept soft labels (e.g., cross_entropy)
    """
    if alpha <= 0:
        # No MixUp, just use default collation
        return default_collate

    # Create MixUp transform
    mixup = v2.MixUp(num_classes=num_classes, alpha=alpha)

    def collate_fn(batch: list[BatchItem]) -> tuple[torch.Tensor, torch.Tensor]:
        """Collate batch with MixUp augmentation.

        Args:
            batch: List of samples, each containing 'pixel_values' and 'labels'

        Returns:
            Tuple of (images, labels) with MixUp applied.
        """
        # Extract images and labels
        images = torch.stack([item["pixel_values"] for item in batch])
        labels = torch.stack([item["labels"] for item in batch])

        # Apply MixUp (handles one-hot encoding internally if labels are 1D)
        images, labels = mixup(images, labels)

        return images, labels

    return collate_fn
